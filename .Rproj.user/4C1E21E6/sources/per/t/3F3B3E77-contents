---
title: "Polls"
output: html_notebook
---

```{r}
library(tidyverse)
library(dplyr)
library(dslabs)
```

# Spread
sample proportion: p
spread: p - (1 - p) = 2p - 1
se for spread: 2 * sqrt(p(1-p)/N)

Notice that most polls that fail to include p are underestimating. The rationale for this is that undecided voters historically divide evenly between the two main candidates on election day. In this case, it is more informative to estimate the spread or the difference between the proportion of two candidates , or for this election.

Assume that there are only two parties. Construct a 95% confidence interval for difference in proportions on election night.
```{r}
data(polls_us_election_2016)

# Create a table called `polls` that filters by  state, date, and reports the spread
polls <- polls_us_election_2016 %>% 
  filter(state != "U.S." & enddate >= "2016-10-31") %>% 
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)

# Create an object called `cis` that columns for the lower and upper confidence intervals. Select the columns indicated in the instructions.
cis <- polls %>% mutate(X_hat = (spread+1)/2, se = 2*sqrt(X_hat*(1-X_hat)/samplesize), 
                 lower = spread - qnorm(0.975)*se, upper = spread + qnorm(0.975)*se) %>% 
  # interval <- spread + c(-1,1)*qnorm(0.975)*se_hat
  select(state, startdate, enddate, pollster, grade, spread, lower, upper) 
```

```{r}
# Add the actual results to the `cis` data set
add <- results_us_election_2016 %>% mutate(actual_spread = clinton/100 - trump/100) %>% select(state, actual_spread)
ci_data <- cis %>% mutate(state = as.character(state)) %>% left_join(add, by = "state")

# Create an object called `p_hits` that summarizes the proportion of hits for each pollster that has at least 5 polls. 
p_hits <- ci_data %>% mutate(hit = lower <= actual_spread & upper >= actual_spread) %>% 
  group_by(pollster) %>%
  filter(n() >=  5) %>%
  summarize(proportion_hits = mean(hit), n = n(), grade = grade[1]) %>%
  arrange(desc(proportion_hits))
p_hits
```

# Election forecasting
In our model:
- The spread $d \sim N(\mu, \tau)$ describes our best guess in the absence of polling data. We set $\mu=0$ and $\tau=0.035$ using historical data.
- The average of observed data $\bar{X} \mid d \sim N(d, \sigma)$ describes randomness due to sampling and the pollster effect.
- Because the posterior distribution is normal, we can report a $95 \%$ credible interval that has a $95 \%$ chance of overlapping the parameter using $\mathrm{E}(p \mid Y)$ and $\mathrm{SE}(p \mid Y)$.
- Given an estimate of $\mathrm{E}(p \mid Y)$ and $\mathrm{SE}(p \mid Y),$ we can use pnorm to compute the probability that $d>0$.

```{r}
polls <- polls_us_election_2016 %>%
    filter(state == "U.S." & enddate >= "2016-10-31" &
                 (grade %in% c("A+", "A", "A-", "B+") | is.na(grade))) %>%
    mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)

one_poll_per_pollster <- polls %>% group_by(pollster) %>%
    filter(enddate == max(enddate)) %>%
    ungroup()

results <- one_poll_per_pollster %>%
    summarize(avg = mean(spread), se = sd(spread)/sqrt(length(spread))) %>%
    mutate(start = avg - 1.96*se, end = avg + 1.96*se)
```

```{r}
# Computing the posterior mean, standard error, credible interval and probability
mu <- 0
tau <- 0.035
sigma <- results$se
Y <- results$avg
B <- sigma^2 / (sigma^2 + tau^2)
posterior_mean <- B*mu + (1-B)*Y
posterior_se <- sqrt(1 / (1/sigma^2 + 1/tau^2))

posterior_mean
posterior_se

# 95% credible interval
posterior_mean + c(-1.96, 1.96)*posterior_se

# probability of d > 0
1 - pnorm(0, posterior_mean, posterior_se)
```

It is common to see a general bias that affects all pollsters in the same way. This bias cannot be predicted or measured before the election. We will include a term in later models to account for this variability.

- If we collect several polls with measured spreads $X_{1}, \ldots, X_{j}$ with a sample size of $N,$ these random variables have expected value $d$ and standard error $2 \sqrt{p(1-p) / N}$
- We represent each measurement as $X_{i, j}=d+b+h_{i}+\epsilon_{i, j}$ where:
- The index $i$ represents the different pollsters
- The index $j$ represents the different polls
- $X_{i, j}$ is the $j$ th poll by the $i$ th pollster
- $d$ is the actual spread of the election
- $b$ is the general bias affecting all pollsters
- $h_{i}$ represents the house effect for the $i$ th pollster
- $\epsilon_{i, j}$ represents the random error associated with the $i, j$ th poll.
- The sample average is now $\bar{X}=d+b+\frac{1}{N} \sum_{i=1}^{N} X_{i}$ with standard deviation $\operatorname{SE}(\bar{X})=\sqrt{\sigma^{2} / N+\sigma_{b}^{2}}$
- The standard error of the general bias $\sigma_{b}$ does not get reduced by averaging multiple polls, which increases the variability of our final estimate.
```{r}
# Calculating probability of d > 0  with general bias
mu <- 0
tau <- 0.035
sigma <- sqrt(results$se^2 + .025^2)
Y <- results$avg
B <- sigma^2 / (sigma^2 + tau^2)

posterior_mean <- B*mu + (1-B)*Y
posterior_se <- sqrt(1 / (1/sigma^2 + 1/tau^2))

1 - pnorm(0, posterior_mean, posterior_se)
```

In the US election, each state has a certain number of votes that are won all-or-nothing based on the popular vote result in that state (with minor exceptions not discussed here). We use the left_join() function to combine the number of electoral votes with our poll results.
For each state, we apply a Bayesian approach to generate an Election Day d. We keep our prior simple by assuming an expected value of 0 and a standard deviation based on recent history of 0.02.

We can run a Monte Carlo simulation that for each iteration simulates poll results in each state using that state's average and standard deviation, awards electoral votes for each state to Clinton if the spread is greater than 0, then compares the number of electoral votes won to the number of votes required to win the election (over 269).

If we run a Monte Carlo simulation for the electoral college without accounting for general bias, we overestimate Clinton's chances of winning at over 99%. If we include a general bias term, the estimated probability of Clinton winning decreases significantly.

```{r}
# Computing the average and standard deviation for each state
results <- polls_us_election_2016 %>%
    filter(state != "U.S." &
            !grepl("CD", "state") &
            enddate >= "2016-10-31" &
            (grade %in% c("A+", "A", "A-", "B+") | is.na(grade))) %>%
    mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) %>%
    group_by(state) %>%
    summarize(avg = mean(spread), sd = sd(spread), n = n()) %>%
    mutate(state = as.character(state))

# 10 closest races = battleground states
results %>% arrange(abs(avg))

# joining electoral college votes and results
results <- left_join(results, results_us_election_2016, by="state")

# states with no polls: note Rhode Island and District of Columbia = Democrat
results_us_election_2016 %>% filter(!state %in% results$state)

# assigns sd to states with just one poll as median of other sd values
results <- results %>%
    mutate(sd = ifelse(is.na(sd), median(results$sd, na.rm = TRUE), sd))
```
```{r}
# Calculating the posterior mean and posterior standard error
mu <- 0
tau <- 0.02
results %>% mutate(sigma = sd/sqrt(n),
                   B = sigma^2/ (sigma^2 + tau^2),
                   posterior_mean = B*mu + (1-B)*avg,
                   posterior_se = sqrt( 1 / (1/sigma^2 + 1/tau^2))) %>%
    arrange(abs(posterior_mean))

```

```{r}
# Monte Carlo simulation including general bias
mu <- 0
tau <- 0.02
bias_sd <- 0.03
clinton_EV_2 <- replicate(1000, {
    results %>% mutate(sigma = sqrt(sd^2/(n) + bias_sd^2),    # added bias_sd term
                        B = sigma^2/ (sigma^2 + tau^2),
                        posterior_mean = B*mu + (1-B)*avg,
                        posterior_se = sqrt( 1 / (1/sigma^2 + 1/tau^2)),
                        simulated_result = rnorm(length(posterior_mean), posterior_mean, posterior_se),
                        clinton = ifelse(simulated_result > 0, electoral_votes, 0)) %>%    # award votes if Clinton wins state
        summarize(clinton = sum(clinton)) %>%    # total votes for Clinton
        .$clinton + 7    # 7 votes for Rhode Island and DC
})
mean(clinton_EV_2 > 269)    # over 269 votes wins election

```

- In poll results, $p$ is not fixed over time. Variability within a single pollster comes from time variation.
- In order to forecast, our model must include a bias term $b_{t}$ to model the time effect.
- Pollsters also try to estimate $f(t),$ the trend of $p$ given time $t$ using a model like:
$$
Y_{i, j, t}=d+b+h_{j}+b_{t}+f(t)+\epsilon_{i, j, t}
$$
- Once we decide on a model, we can use historical data and current data to estimate the necessary parameters to make predictions.

```{r}
# Variability across one pollster

# select all national polls by one pollster
one_pollster <- polls_us_election_2016 %>%
    filter(pollster == "Ipsos" & state == "U.S.") %>%
    mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)

# the observed standard error is higher than theory predicts
se <- one_pollster %>%
    summarize(empirical = sd(spread),
            theoretical = 2*sqrt(mean(spread)*(1-mean(spread))/min(samplesize)))
se

# the distribution of the data is not normal
one_pollster %>% ggplot(aes(spread)) +
    geom_histogram(binwidth = 0.01, color = "black")
```

```{r}
# Trend across time for several pollsters
polls_us_election_2016 %>%
    filter(state == "U.S." & enddate >= "2016-07-01") %>%
    group_by(pollster) %>%
    filter(n() >= 10) %>%
    ungroup() %>%
    mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) %>%
    ggplot(aes(enddate, spread)) +
    geom_smooth(method = "loess", span = 0.1) +
    geom_point(aes(color = pollster), show.legend = FALSE, alpha = 0.6)

# Plotting raw percentages across time
polls_us_election_2016 %>%
    filter(state == "U.S." & enddate >= "2016-07-01") %>%
    select(enddate, pollster, rawpoll_clinton, rawpoll_trump) %>%
    rename(Clinton = rawpoll_clinton, Trump = rawpoll_trump) %>%
    gather(candidate, percentage, -enddate, -pollster) %>%
    mutate(candidate = factor(candidate, levels = c("Trump", "Clinton"))) %>%
    group_by(pollster) %>%
    filter(n() >= 10) %>%
    ungroup() %>%
    ggplot(aes(enddate, percentage, color = candidate)) +
    geom_point(show.legend = FALSE, alpha = 0.4) +
    geom_smooth(method = "loess", span = 0.15) +
    scale_y_continuous(limits = c(30, 50))
```

